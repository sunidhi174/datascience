{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anomaly Detection in Claims Submission Process\n",
    "======\n",
    "\n",
    "### Agenda of this post is: \n",
    "* Business Problem Overview\n",
    "* Summary of Model Output\n",
    "* Factspan's Approach Framework\n",
    "* Key Challenges in the recommended approach\n",
    "* Exploratory Data Analysis\n",
    "* Anomaly Detection Algorithm Development\n",
    "    * Risk Assessment Framework\n",
    "    * Isolation Forest overview\n",
    "    * Python Implementation Demo\n",
    "* Long Term Vision\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Problem Overview:\n",
    "\n",
    "** Current State **\n",
    "* Client maintains order-to-cash claims data in SAP system\n",
    "* They scrutinize every claim that are filed by different customers to **manually identify** fraudulent cases\n",
    "* Manual effort lead to:\n",
    "    * Longer processing time\n",
    "    * Less time for analysis and accurate decision making\n",
    "<br>\n",
    "\n",
    "** Problem Statement **\n",
    "* Build a risk assessement and scoring model which would help identify the anomalous claims\n",
    "\n",
    "<br>\n",
    "** Desired State **\n",
    "* Client is able to detect the anomaly quickly\n",
    "* Client is able to manage the data in SAP for each claim with risk score\n",
    "* Based on the risk scores, the team is able to prioritize the high risk claims which will enable them to \n",
    "    * Act pro-active, fast and close the claims quickly\n",
    "    * Identify opportunities for recoverability and reduce dollars adjustment towards fraudulent claims\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Model Output\n",
    "####  Implementing Factspan's anomaly detection algorithm can help Client prioritize high risk claims worth \\$45.2 Million for investigation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div><div style = \"text-align: left\"><img src=\"./Downloads/riskdist.png\" width=\"400\" height = \"300\" style = \"float: left\" /></div><div style = \"text-align: center\"><h1>YoY Comparison at overall level</h1><img src=\"./Downloads/yoy.png\" width=\"450\" height = \"300\" style = \"float: center\" /></div></div><br><br><br>\n",
    "\n",
    "\n",
    "\n",
    "#### \\* The high risk claims (VH + H) account for 0.4% of total claims, but result in around 15% of total claim amount filed for"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Factspan's Step-by-Step Approach to Machine learning\n",
    "___\n",
    "\n",
    "![image.png](./Downloads/approachframework.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Challenges\n",
    "___\n",
    "![Key Challenges](./Downloads/keychallenges.png)\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Proposed Solution](./Downloads/proposedsolution1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Data cleaning and processing resulted in ~670k records for modelling\n",
    "___\n",
    "<img src = \"./Downloads/datacleaning.png\" width = \"600\" height = \"150\" style = \"float: left\"><img src = \"./Downloads/datasummary.png\" width = \"300\" height = \"150\" style = \"float: right\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High YoY drop in Total Claims, Claim Amt, Avg. Amt per claim\n",
    "___\n",
    "<img src = \"./Downloads/higheda.png\" width = \"370\" height = \"150\" style = \"float: left\">\n",
    "<br> <br>\n",
    "### There is a *substantial drop* in total claim amount *YoY* with 60% drop in total claim amount and 16% drop in Total Claims & 52% in Avg. Claim amount\n",
    "### There are 4% lesser customers and 5% lesser distinct reasons for the claims filed in FY 2017 v/s FY 2016\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anomaly Detection Model Development\n",
    "___\n",
    "### Risk Assesment Framework\n",
    "\n",
    "**Objective of the Machine Learning model is to determine *potential risk* of a claim being fraudulent understand its key drivers, and estimate the monetary value attached to such fraudelent claims**\n",
    "\n",
    "![Risk Assessment framework](./Downloads/riskassement.png)\n",
    "<br>\n",
    "**The unsupervised Machine Learning model identifies anomalous claims, and returns a risk score against each claim, that can be further drilled down to identify key factors behind high risk claims, and its business impact**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Isolation Forest Overview\n",
    "___\n",
    "Isolation Forest isolates observations by randomly selecting a feature (variable) and then randomly selecting a split of the selected feature, till all the instances are covered and lie in their own seperate node, hence growing a decision tree.\n",
    "<br>\n",
    "<img src = \"./Downloads/isolation.png\" width = \"600\" height = \"150\" style = \"float: left\">\n",
    "<br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>\n",
    "** Key Feature **\n",
    "\n",
    "* Does not require distance or density measures to detect anomalies\n",
    "* Is a proper binary tree\n",
    "* Underlying assumption - Isolating anomaly observations is easier, as only few conditions are needed to separate abnormal cases from the normal ones\n",
    "* Has linear time complexity, and low memory requirements\n",
    "* Capability to scale up to handle extremely large data, and high-dimensional problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Isolation Forest Python Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Isolation forest is implemented in various libraries but using sci-kit learn APIs makes life easier and it integrates really well with Pandas API and Numpy API**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "folder  ='D:/Nike Data/Modelling Data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Libraries to include\n",
    "* **Pandas** and **Numpy** for handling data in a faster and efficient way\n",
    "* **Matplotlib** for Graphing\n",
    "* **Sklearn** for using machine learning implementations in python\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time as time\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import IsolationForest\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the data using Pandas API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(folder+'input_data1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SoldToCustomer           0\n",
       "RefKey1                  0\n",
       "TotalClaims              0\n",
       "RsnCodeDesc              0\n",
       "Category              4253\n",
       "PlntNm                   0\n",
       "DistribMthdCd            0\n",
       "Frequency_Customer       0\n",
       "RunTot_Customer          0\n",
       "AvgRunTot_Cust           0\n",
       "recency_Customer         0\n",
       "Frequency_Plant          0\n",
       "RunTot_Plant             0\n",
       "AvgRunTot_Plant          0\n",
       "Frequency_Rsn            0\n",
       "RunTot_Rsn               0\n",
       "AvgRunTot_Rsn            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Category has around 4253 Missing values of ~670k records <br>\n",
    "Imputing Missing values with a new category called Unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SoldToCustomer        0\n",
      "RefKey1               0\n",
      "TotalClaims           0\n",
      "RsnCodeDesc           0\n",
      "Category              0\n",
      "PlntNm                0\n",
      "DistribMthdCd         0\n",
      "Frequency_Customer    0\n",
      "RunTot_Customer       0\n",
      "AvgRunTot_Cust        0\n",
      "recency_Customer      0\n",
      "Frequency_Plant       0\n",
      "RunTot_Plant          0\n",
      "AvgRunTot_Plant       0\n",
      "Frequency_Rsn         0\n",
      "RunTot_Rsn            0\n",
      "AvgRunTot_Rsn         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "feature = ['Category']\n",
    "for feature in feature:\n",
    "    data[feature] = data[feature].fillna('Unknown')\n",
    "print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using sklearn's preprocessing API we encode the features of the categorical variables. We need to do this because sklearn's machine learning API doesn't support strings as inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "def encode_features(df_train):\n",
    "    features = ['SoldToCustomer','RsnCodeDesc', 'Category','PlntNm', 'DistribMthdCd']\n",
    "    \n",
    "    for feature in features:\n",
    "        le = preprocessing.LabelEncoder()\n",
    "        le = le.fit(df_train[feature])\n",
    "        df_train[feature] = le.transform(df_train[feature])\n",
    "    return df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.7506749629974365\n"
     ]
    }
   ],
   "source": [
    "st = time.time()\n",
    "data = encode_features(data)\n",
    "print(time.time()-st)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We initate the SKlearn's implementation of Isolation forest.**\n",
    "* n_estimators -> Number of isolation trees to grow\n",
    "* max_samples -> proporation of total data to be included in each isolation trees\n",
    "* n_jobs -> multithreading the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initiating the Algorithm\n",
    "from sklearn.ensemble import IsolationForest\n",
    "clust = IsolationForest(n_estimators = 100, max_samples = 0.5, n_jobs = -1, random_state = 23,verbose = 1, bootstrap = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we drop take a copy of keys and store it in seperate variable and drop it from the dataset since it isn't a feature on which model will learn to detect anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Preparing Data\n",
    "key = data['RefKey1']\n",
    "X = data.drop(['RefKey1'], axis = 1)\n",
    "try: del data\n",
    "except: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:   13.0s remaining:   13.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:   13.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time to fit model: 47.278727293014526 s\n"
     ]
    }
   ],
   "source": [
    "#Fitting the Model\n",
    "st = time.time()\n",
    "clust.fit(X)\n",
    "print(\"time to fit model: {} s\" .format(time.time()-st))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time to make prediction: 65.0640001296997 s\n"
     ]
    }
   ],
   "source": [
    "#Predicting the Anomalies and score\n",
    "st = time.time()\n",
    "Y =clust.predict(X)\n",
    "Y_score = clust.decision_function(X)\n",
    "print(\"time to make prediction: {} s\" .format(time.time()-st))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Outputing the Fil as a csv\n",
    "Y = pd.DataFrame({'Claim ID':key,'Anomaly':Y, 'Risk_Score':Y_score})\n",
    "Y.to_csv(folder+'Anomaly.csv', sep = ',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Long term vision for Client's claim settlement process to maximize the utilization of Advanced Machine Learning algorithms\n",
    "___\n",
    "![Long Term Vision](./Downloads/vision1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
